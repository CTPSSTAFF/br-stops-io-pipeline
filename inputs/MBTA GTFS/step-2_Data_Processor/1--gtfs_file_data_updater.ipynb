{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a086e0e",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "946cf1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the path to the GTFS directory\n",
    "# original_gtfs_dir = '../1--Original_2018_GTFS_Recap_Data/GTFS_Recap_-_Fall_2018-20250626T135819Z-1-001/GTFS_Recap_-_Fall_2018' # Assuming the GTFS files are in this directory\n",
    "original_gtfs_dir = '../1--Original_2018_GTFS_Recap_Data/GTFS_Recap_-_Fall_2018' # Assuming the GTFS files are in this directory\n",
    "\n",
    "new_gtfs_dir = '../4--Updated_GTFS_Files' # Directory where the updated GTFS files will be saved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379a975f",
   "metadata": {},
   "source": [
    "# Copy Old GTFS to Updated GTFS Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "951be2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying '../1--Original_2018_GTFS_Recap_Data/GTFS_Recap_-_Fall_2018' to '../4--Updated_GTFS_Files'...\n",
      "Copy complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil # Import the shutil module for high-level file operations\n",
    "\n",
    "try:\n",
    "    # --- Step 1: Copy the entire original GTFS directory to the new location ---\n",
    "    if os.path.exists(new_gtfs_dir):\n",
    "        print(f\"Warning: '{new_gtfs_dir}' already exists. Deleting its contents to ensure a fresh copy.\")\n",
    "        shutil.rmtree(new_gtfs_dir) # Remove the directory and its contents\n",
    "\n",
    "    print(f\"Copying '{original_gtfs_dir}' to '{new_gtfs_dir}'...\")\n",
    "    shutil.copytree(original_gtfs_dir, new_gtfs_dir)\n",
    "    print(\"Copy complete.\")\n",
    "\n",
    "    # Now, all subsequent operations will be on the files in new_gtfs_dir\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Please ensure the 'MBTA_2018' directory exists and contains 'calendar.txt' (and optionally 'calendar_dates.txt').\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988ffe7e",
   "metadata": {},
   "source": [
    "## Replace STOP ids in updated GTFS from MBTA_Stopid_lookup_updated.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bb8694b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded '..\\2--Stopid_Processor\\matched_files_with_stop_id_columns.csv' with 5 entries.\n",
      "Loaded '..\\2--Stopid_Processor\\MBTA_Stopid_lookup_updated.csv' with 10904 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\tdm23_env_1\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3361: DtypeWarning: Columns (0,3,9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop ID update log created at '.\\stopid_update_log.txt'.\n"
     ]
    }
   ],
   "source": [
    "# Define CSV paths (relative to the current directory)\n",
    "matched_files_csv = os.path.join(\"..\", \"2--Stopid_Processor\", \"matched_files_with_stop_id_columns.csv\")\n",
    "lookup_csv = os.path.join(\"..\", \"2--Stopid_Processor\", \"MBTA_Stopid_lookup_updated.csv\")\n",
    "\n",
    "# Read the CSVs if not already loaded\n",
    "matched_files_df = pd.read_csv(matched_files_csv)\n",
    "print(f\"Loaded '{matched_files_csv}' with {len(matched_files_df)} entries.\")\n",
    "\n",
    "lookup_df = pd.read_csv(lookup_csv)\n",
    "print(f\"Loaded '{lookup_csv}' with {len(lookup_df)} entries.\")\n",
    "\n",
    "# Create mapping dictionary from lookup_df using 'stop_id' and 'stop_id_update'\n",
    "if 'stop_id' in lookup_df.columns and 'stop_id_update' in lookup_df.columns:\n",
    "    mapping_dict = dict(zip(lookup_df['stop_id'], lookup_df['stop_id_update']))\n",
    "else:\n",
    "    mapping_dict = dict(zip(lookup_df.iloc[:, 1], lookup_df.iloc[:, 5]))\n",
    "\n",
    "# Log list to accumulate summary info.\n",
    "log_lines = []\n",
    "log_lines.append(\"Stop ID Update Log\\n\")\n",
    "log_lines.append(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Process each file listed in the matched files CSV.\n",
    "# matched_files_df is assumed to have columns 'file_name' and 'stop_id_columns'\n",
    "for _, row in matched_files_df.iterrows():\n",
    "    file_name = row['file_name']\n",
    "    # Split the stop id column names in case multiple are provided as comma-separated\n",
    "    stop_id_columns = [col.strip() for col in row['stop_id_columns'].split(',')]\n",
    "    file_path = os.path.join(new_gtfs_dir, file_name)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        log_lines.append(f\"File '{file_path}' not found. Skipping.\\n\")\n",
    "        continue\n",
    "    \n",
    "    # Correct the variable name: use file_path instead of non-existent file_canpath.\n",
    "    df = pd.read_csv(file_path)\n",
    "    file_updated = False\n",
    "    log_lines.append(f\"\\nProcessing file: {file_name}\\n\")\n",
    "    for col in stop_id_columns:\n",
    "        if col in df.columns:\n",
    "            # Convert the values to string and update using mapping_dict\n",
    "            original_col = df[col].astype(str)\n",
    "            updated_col = original_col.map(mapping_dict).fillna(original_col)\n",
    "            # Create a boolean mask for rows where the stop id was changed.\n",
    "            mask = original_col != updated_col\n",
    "            changed_count = mask.sum()\n",
    "            if changed_count > 0:\n",
    "                file_updated = True\n",
    "                # Create a summary of unique changes.\n",
    "                changes = pd.DataFrame({'old': original_col[mask], 'new': updated_col[mask]})\n",
    "                unique_changes = changes.drop_duplicates().sort_values(by='old')\n",
    "                log_lines.append(f\"In column '{col}': {changed_count} rows changed.\\n\")\n",
    "                log_lines.append(unique_changes.to_string(index=False) + \"\\n\")\n",
    "            else:\n",
    "                log_lines.append(f\"In column '{col}': no changes made.\\n\")\n",
    "            \n",
    "            # Update the dataframe column.\n",
    "            df[col] = updated_col\n",
    "        else:\n",
    "            log_lines.append(f\"Column '{col}' not found in '{file_name}'.\\n\")\n",
    "    \n",
    "    if file_updated:\n",
    "        df.to_csv(file_path, index=False)\n",
    "        log_lines.append(f\"File '{file_name}' updated and saved successfully.\\n\")\n",
    "    else:\n",
    "        log_lines.append(f\"No updates made to '{file_name}'.\\n\")\n",
    "\n",
    "# Write the log file in the current directory.\n",
    "log_file_path = os.path.join(\".\", \"stopid_update_log.txt\")\n",
    "with open(log_file_path, \"w\") as f:\n",
    "    f.writelines(log_lines)\n",
    "print(f\"Stop ID update log created at '{log_file_path}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b738f7",
   "metadata": {},
   "source": [
    "## Update 2018 GTFS year to match Scenario GTFS date/year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8e326cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to prepare and update GTFS calendar files from '../1--Original_2018_GTFS_Recap_Data/GTFS_Recap_-_Fall_2018' for '../4--Updated_GTFS_Files'.\n",
      "\n",
      "Loaded calendar.txt from '../4--Updated_GTFS_Files' with 119 entries.\n",
      "Updated calendar.txt saved to '../4--Updated_GTFS_Files\\calendar.txt'.\n",
      "Loaded calendar_dates.txt from '../4--Updated_GTFS_Files' with 618 entries.\n",
      "Updated calendar_dates.txt saved to '../4--Updated_GTFS_Files\\calendar_dates.txt'.\n",
      "\n",
      "GTFS calendar files successfully updated to 2024 and saved in the '../4--Updated_GTFS_Files' directory.\n",
      "Your original GTFS files in 'MBTA_2018' remain unchanged.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Attempting to prepare and update GTFS calendar files from '{original_gtfs_dir}' for '{new_gtfs_dir}'.\\n\")\n",
    "\n",
    "# --- Update calendar.txt ---\n",
    "# We now operate on the files in the new_gtfs_dir\n",
    "calendar_path = os.path.join(new_gtfs_dir, 'calendar.txt')\n",
    "\n",
    "if not os.path.exists(calendar_path):\n",
    "    raise FileNotFoundError(f\"'{calendar_path}' not found in the new directory. Copy might have failed or original is missing.\")\n",
    "\n",
    "calendar_df = pd.read_csv(calendar_path)\n",
    "print(f\"Loaded calendar.txt from '{new_gtfs_dir}' with {len(calendar_df)} entries.\")\n",
    "\n",
    "# Function to update the year in a date string\n",
    "def update_year_to_2024(date_str):\n",
    "    if pd.isna(date_str):\n",
    "        return date_str\n",
    "    date_str = str(date_str) # Ensure it's a string\n",
    "    if len(date_str) == 8 and date_str.isdigit():\n",
    "        # Assuming YYYYMMDD format\n",
    "        original_year = date_str[:4]\n",
    "        if original_year == '2018': # Only replace if it's 2018\n",
    "            return '2024' + date_str[4:]\n",
    "    return date_str\n",
    "\n",
    "# Apply the update function to start_date and end_date columns\n",
    "calendar_df['start_date'] = calendar_df['start_date'].apply(update_year_to_2024)\n",
    "calendar_df['end_date'] = calendar_df['end_date'].apply(update_year_to_2024)\n",
    "\n",
    "# Save the updated calendar.txt back to the new directory (overwriting the copied one)\n",
    "calendar_df.to_csv(calendar_path, index=False)\n",
    "print(f\"Updated calendar.txt saved to '{calendar_path}'.\")\n",
    "\n",
    "# --- Update calendar_dates.txt ---\n",
    "calendar_dates_path = os.path.join(new_gtfs_dir, 'calendar_dates.txt')\n",
    "\n",
    "if os.path.exists(calendar_dates_path):\n",
    "    calendar_dates_df = pd.read_csv(calendar_dates_path)\n",
    "    print(f\"Loaded calendar_dates.txt from '{new_gtfs_dir}' with {len(calendar_dates_df)} entries.\")\n",
    "\n",
    "    # Apply the update function to the 'date' column\n",
    "    calendar_dates_df['date'] = calendar_dates_df['date'].apply(update_year_to_2024)\n",
    "\n",
    "    # Save the updated calendar_dates.txt back to the new directory (overwriting the copied one)\n",
    "    calendar_dates_df.to_csv(calendar_dates_path, index=False)\n",
    "    print(f\"Updated calendar_dates.txt saved to '{calendar_dates_path}'.\")\n",
    "else:\n",
    "    print(f\"'{calendar_dates_path}' not found in the new directory. Skipping update for this file (it's optional).\")\n",
    "\n",
    "print(f\"\\nGTFS calendar files successfully updated to 2024 and saved in the '{new_gtfs_dir}' directory.\")\n",
    "print(\"Your original GTFS files in 'MBTA_2018' remain unchanged.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58076be4",
   "metadata": {},
   "source": [
    "### Identify in stops.txt, identify and remove stops with missing Lat Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b8b6f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to remove stops with missing latitude/longitude from stops.txt in '../4--Updated_GTFS_Files'.\n",
      "\n",
      "Loaded stops.txt with 9237 entries.\n",
      "\n",
      "Found 588 stops with missing latitude or longitude.\n",
      "These stops will be removed:\n",
      "     stop_id stop_name  stop_lat  stop_lon\n",
      "7800   C1260    Andrew       NaN       NaN\n",
      "7801   C1261    Andrew       NaN       NaN\n",
      "7802   C1271    Andrew       NaN       NaN\n",
      "7803   C1343    Andrew       NaN       NaN\n",
      "7804   C1344    Andrew       NaN       NaN\n",
      "\n",
      "Removed 588 rows. Keeping 8649 rows.\n",
      "Cleaned stops.txt saved successfully, overwriting the original file at '../4--Updated_GTFS_Files\\stops.txt'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(f\"Attempting to remove stops with missing latitude/longitude from stops.txt in '{new_gtfs_dir}'.\\n\")\n",
    "\n",
    "try:\n",
    "    # Define the path to stops.txt\n",
    "    stops_path = os.path.join(new_gtfs_dir, 'stops.txt')\n",
    "\n",
    "    if not os.path.exists(stops_path):\n",
    "        raise FileNotFoundError(f\"'{stops_path}' not found. Please ensure the GTFS files are in the correct directory.\")\n",
    "\n",
    "    # Load stops.txt\n",
    "    stops_df = pd.read_csv(stops_path)\n",
    "    print(f\"Loaded stops.txt with {len(stops_df)} entries.\")\n",
    "\n",
    "    # Identify rows where 'stop_lat' or 'stop_lon' are NaN (missing)\n",
    "    # The .isna() method checks for NaN values.\n",
    "    # We use | (OR) to find rows where EITHER lat OR lon is missing.\n",
    "    stops_with_missing_latlong = stops_df[stops_df['stop_lat'].isna() | stops_df['stop_lon'].isna()]\n",
    "    num_missing = len(stops_with_missing_latlong)\n",
    "\n",
    "    if num_missing > 0:\n",
    "        print(f\"\\nFound {num_missing} stops with missing latitude or longitude.\")\n",
    "        print(\"These stops will be removed:\")\n",
    "        # Displaying a sample of the stops to be removed\n",
    "        print(stops_with_missing_latlong[['stop_id', 'stop_name', 'stop_lat', 'stop_lon']].head())\n",
    "\n",
    "        # Remove rows where 'stop_lat' or 'stop_lon' are NaN\n",
    "        # The ~ (NOT) operator inverts the boolean mask, keeping rows where NEITHER is NaN.\n",
    "        cleaned_stops_df = stops_df.dropna(subset=['stop_lat', 'stop_lon'])\n",
    "        num_kept = len(cleaned_stops_df)\n",
    "\n",
    "        print(f\"\\nRemoved {num_missing} rows. Keeping {num_kept} rows.\")\n",
    "\n",
    "        # Overwrite the original stops.txt with the cleaned DataFrame\n",
    "        cleaned_stops_df.to_csv(stops_path, index=False)\n",
    "        print(f\"Cleaned stops.txt saved successfully, overwriting the original file at '{stops_path}'.\")\n",
    "    else:\n",
    "        print(\"\\nNo stops with missing latitude or longitude were found. No changes made to stops.txt.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Please ensure the 'MBTA_2018_gtfs' directory exists and contains 'stops.txt'.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdm23_env_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
